{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes taken from https://gym.openai.com/docs/\n",
    "\n",
    "#Lunar Lander\n",
    "#The Discrete space allows a fixed range of non-negative numbers (env.action_space). \n",
    "#In this case valid actions are 4 (left, right, fire engine, do nothing)\n",
    "\n",
    "#The Box space represents an n-dimensional box, valid observations will be an array of 8 numbers.\n",
    "#Which are these numbers?  (Ref. https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py)\n",
    "#(1-2) position in x axis and y axis(height)\n",
    "#(3-4) x,y axis velocity terms\n",
    "#(5-6) lander angle and angular velocity\n",
    "#(7-8) left and right contact points (bool -> 1 in case on contact, 0 otherwise)\n",
    "\n",
    "#We can also check the Boxâ€™s bounds (env.obs_space): from -inf to +inf  [Box(-inf, inf, (8,), float32)]\n",
    "#print the structure of the observations your environment will be returning. Learning agents usually\n",
    "#need to know this before they start running, in order to set up the policy function.\n",
    "\n",
    "#hyperparameters partially taken from https://arxiv.org/pdf/2011.11850.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gnwrapper\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from keras.activations import relu, linear\n",
    "\n",
    "env = gnwrapper.LoopAnimation(gym.make('LunarLander-v2'))\n",
    "\n",
    "print(env.observation_space)  \n",
    "print(env.action_space) \n",
    "\n",
    "#otherwise the agent would always train on similar scenarios\n",
    "env.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ddqn:\n",
    "\n",
    "    def __init__(self, action_space, state_space):\n",
    "        #hyperparameters\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.996                   \n",
    "        self.epsilon_min = 0.01                      \n",
    "        self.discount = 0.99\n",
    "        self.batch_size = 64                         \n",
    "        self.lr = 0.001                       \n",
    "        self.memory = deque(maxlen=1000000)\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_freq = 100\n",
    "        self.counter = 0\n",
    "\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim= 8, activation=relu))\n",
    "        model.add(Dense(128, activation=relu))\n",
    "        model.add(Dense(4, activation=linear))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.lr))\n",
    "        return model \n",
    "    \n",
    "    #hard update method (every n-step)\n",
    "    def update_target_model(self):\n",
    "        self.counter += 1\n",
    "        if self.counter >= self.update_target_freq:\n",
    "            print('Updating weights...')\n",
    "            self.counter = 0\n",
    "            #overwrite target_model weights with the ones from main model\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "        return  \n",
    "    \n",
    "    #append the acquired experience into the memory\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    #based on e-greedy policy\n",
    "    #if random is smaller than e, take a random action, otherwise returns the index of maximum Q-value\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    #trains the model using randomly selected experiences in the replay memory\n",
    "    def learn_from_exp(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        \n",
    "        minibatch = random.sample(self.memory, self.batch_size)        \n",
    "    \n",
    "        for i in minibatch:\n",
    "            states.append(i[0])\n",
    "            actions.append(i[1])\n",
    "            rewards.append(i[2])\n",
    "            next_states.append(i[3])\n",
    "            dones.append(i[4])\n",
    "\n",
    "        #reducing the dimension of states/next_states np.arrays\n",
    "        states = np.squeeze(np.array(states))\n",
    "        next_states = np.squeeze(np.array(next_states))\n",
    "\n",
    "        main_pred = self.model.predict(states)\n",
    "        main_next_pred = self.model.predict(next_states)\n",
    "        \n",
    "        target_next_pred = self.target_model.predict(next_states) \n",
    "            \n",
    "        for i in range(self.batch_size):\n",
    "            #Q value correction based on the taken action\n",
    "            if dones[i]:\n",
    "                main_pred[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                # current Q Network selects the actions\n",
    "                # target Q Network evaluates the actions\n",
    "                # max Q -> Q_main_pred(s', argmax_a' Q(s', a'))\n",
    "                main_pred[i][actions[i]]=rewards[i]+self.discount*(target_next_pred[i][np.argmax(main_next_pred[i])])\n",
    "                \n",
    "        #fit -> updating Q-values in main_pred for the actual states (1 iteration)\n",
    "        self.model.fit(states, main_pred, epochs=1, verbose=0) \n",
    "        \n",
    "        #performing a gradient descent step\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ddqn(env.action_space.n, env.observation_space.shape[0])\n",
    "rewards = []\n",
    "m_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(episode):\n",
    "\n",
    "    global rewards\n",
    "    global m_rewards\n",
    "    \n",
    "    for e in range(episode):\n",
    "        #environment reset\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, 8))\n",
    "        #initializing our score, steps\n",
    "        score = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            steps += 1\n",
    "            #if random is smaller than e, take a random action, otherwise returns the index of maximum Q-value\n",
    "            action = agent.act(state)\n",
    "            #from env.step(action), take these 3 values (ignoring \"info\")\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, 8))\n",
    "            #append in memory\n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "            #go to next state\n",
    "            state = next_state\n",
    "            #trains the model using randomly selected experiences in the learn memory\n",
    "            #performs a gradient descend step\n",
    "            agent.learn_from_exp()\n",
    "            agent.update_target_model()\n",
    "            if done:\n",
    "                print(\"\\nEpisode {}/{} concluded after {} steps. Score: {:.3f}\".format(e, episode, steps, score))\n",
    "                break\n",
    "        rewards.append(score)\n",
    "        m_rewards.append(np.mean(rewards[-100:]))\n",
    "\n",
    "        #Mean reward on last 100 episode\n",
    "        if m_rewards[-1] > 200:\n",
    "            print('\\n ### Train has finished ###\\n\\n Final mean reward: {:.3f} \\n'.format(m_rewards[-1]))\n",
    "            break\n",
    "        print(\"Mean reward (last 100 episodes): {:.3f} \\n\".format(m_rewards[-1]))\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_training = training(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of the results\n",
    "\n",
    "plt.plot(rewards, label='Double DQN', linewidth= '1')\n",
    "plt.plot(m_rewards, label='Average reward', color = 'r', linewidth='3')\n",
    "\n",
    "#saving the curves\n",
    "np.savez('rewards_DDQN', rewards)\n",
    "np.savez('m_rewards_DDQN', m_rewards)\n",
    "\n",
    "plt.title('Double DQN')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.grid()\n",
    "plt.legend(loc = 4)\n",
    "plt.savefig('Double_DQN.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the trained model\n",
    "agent.model.save('Double_DQN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#at restore, run this method after executing ln[2-3-4-5].\n",
    "agent.model = tf.keras.models.load_model('Double_DQN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "\n",
    "#uncomment 'env.render()' and 'env.display()' to render the episodes\n",
    "\n",
    "state = env.reset()\n",
    "steps = 0\n",
    "eval_time = 60*15\n",
    "starting_time = time.time()\n",
    "stop = lambda : int(time.time() - starting_time) >= eval_time\n",
    "rewards_eval = []\n",
    "ep_reward = 0\n",
    "\n",
    "while True:\n",
    "    state = np.reshape(state, (1, 8))\n",
    "    #returns index of action with maximum value\n",
    "    action = np.argmax(agent.model.predict(state))\n",
    "    #env.render()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    steps += 1\n",
    "    ep_reward += reward\n",
    "    if done:\n",
    "        rewards_eval.append(ep_reward)\n",
    "        print(\"Episode ended in {} time steps\".format(steps))\n",
    "        #env.display()\n",
    "        steps = 0\n",
    "        ep_reward = 0\n",
    "        state = env.reset()\n",
    "        if stop(): \n",
    "            print('Mean reward after the evaluation: {}'.format(np.mean(rewards_eval)))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
