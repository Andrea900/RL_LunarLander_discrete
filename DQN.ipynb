{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes taken from https://gym.openai.com/docs/\n",
    "\n",
    "#Lunar Lander\n",
    "#The Discrete space allows a fixed range of non-negative numbers (env.action_space). \n",
    "#In this case valid actions are 4 (left, right, fire engine, do nothing)\n",
    "\n",
    "#The Box space represents an n-dimensional box, valid observations will be an array of 8 numbers.\n",
    "#Which are these numbers?  (Ref. https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py)\n",
    "#(1-2) position in x axis and y axis(height)\n",
    "#(3-4) x,y axis velocity terms\n",
    "#(5-6) lander angle and angular velocity\n",
    "#(7-8) left and right contact points (bool -> 1 in case on contact, 0 otherwise)\n",
    "\n",
    "#We can also check the Box’s bounds (env.obs_space): from -inf to +inf  [Box(-inf, inf, (8,), float32)]\n",
    "#print the structure of the observations your environment will be returning. Learning agents usually\n",
    "#need to know this before they start running, in order to set up the policy function.\n",
    "\n",
    "#hyperparameters taken from https://arxiv.org/pdf/2011.11850.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gnwrapper\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from keras.activations import relu, linear\n",
    "\n",
    "env = gnwrapper.LoopAnimation(gym.make('LunarLander-v2'))\n",
    "\n",
    "print(env.observation_space)  \n",
    "print(env.action_space) \n",
    "\n",
    "#otherwise the agent would always train on similar scenarios.\n",
    "env.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn:\n",
    "\n",
    "    def __init__(self, action_space, state_space):\n",
    "        #hyperparameters\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.996                   \n",
    "        self.epsilon_min = 0.01                      \n",
    "        self.discount = 0.99                          \n",
    "        self.batch_size = 64                         \n",
    "        self.lr = 0.001\n",
    "        self.memory = deque(maxlen=1000000)\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=8, activation=relu))\n",
    "        model.add(Dense(128, activation=relu))\n",
    "        model.add(Dense(4, activation=linear))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.lr))\n",
    "        return model\n",
    "\n",
    "    #append the acquired experience into the memory\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    #based on e-greedy policy\n",
    "    #if random is smaller than e, take a random action, otherwise returns the index of maximum Q-value\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    #trains the model using randomly selected experiences in the replay memory\n",
    "    def learn_from_exp(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        \n",
    "        minibatch = random.sample(self.memory, self.batch_size)        \n",
    "    \n",
    "        for i in minibatch:\n",
    "            states.append(i[0])\n",
    "            actions.append(i[1])\n",
    "            rewards.append(i[2])\n",
    "            next_states.append(i[3])\n",
    "            dones.append(i[4])\n",
    "         \n",
    "        #reducing the dimension of states/next_states np.arrays\n",
    "        states = np.squeeze(np.array(states))\n",
    "        next_states = np.squeeze(np.array(next_states))\n",
    "        \n",
    "        main_pred = self.model.predict(states)\n",
    "        main_next_pred = self.model.predict(next_states)\n",
    "        \n",
    "        #sample random minibatch of transitions (Φj ; aj ; rj ; Φj+1) from D\n",
    "        #set yj (main_pred) = rj + gamma*max(a')*Q(Φ(j+1,a';theta))\n",
    "        #(1-np.array(dones)): if next_state -> terminal, current main_pred = np.array(rewards) [dones is a vector composed of 0/1 values]\n",
    "        main_pred = np.array(rewards) + self.discount *(np.amax(main_next_pred, axis=1))*(1-np.array(dones))\n",
    "        \n",
    "        #Taking Qs for the actual states \n",
    "        main_pred_all = self.model.predict(states)\n",
    "        \n",
    "        for i in range(self.batch_size):        \n",
    "        #update on selected Q for specific actions related to a state \n",
    "            main_pred_all[i][actions[i]] = main_pred[i]\n",
    "\n",
    "        #fit -> updating Q-values in main_pred_all for the actual states (1 iteration)\n",
    "        self.model.fit(states, main_pred_all, epochs=1, verbose=0) \n",
    "        \n",
    "        #performing a gradient descent step\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = dqn(env.action_space.n, env.observation_space.shape[0])\n",
    "rewards = []\n",
    "m_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(episode):\n",
    "\n",
    "    global rewards\n",
    "    global m_rewards\n",
    "    \n",
    "    for e in range(episode):\n",
    "        #environment reset\n",
    "        state = env.reset()\n",
    "        #reshape np.array \"state\", 1 row composed by 8 elements(observation_space))\n",
    "        state = np.reshape(state, (1, 8))\n",
    "        #initializing our score and steps counters\n",
    "        score = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            steps += 1\n",
    "            #if random is smaller than e, take a random action, otherwise returns the index of maximum Q-value\n",
    "            action = agent.act(state)\n",
    "            #from env.step(action), take these 3 values (ignoring \"info\")\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, 8))\n",
    "            #append in memory\n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "            #go to next state\n",
    "            state = next_state\n",
    "            #trains the model using randomly selected experiences in the memory\n",
    "            #performs a gradient descend step\n",
    "            agent.learn_from_exp()\n",
    "            if done:\n",
    "                print(\"Episode {}/{} concluded after {} steps. Score: {:.3f}\".format(e, episode, steps, score))\n",
    "                break\n",
    "        rewards.append(score)\n",
    "        m_rewards.append(np.mean(rewards[-100:]))\n",
    "\n",
    "        #Mean reward on last 100 episode\n",
    "        if m_rewards[-1] > 200:\n",
    "            print('\\n ### Train has finished ###\\n\\n Final mean reward: {:.3f} \\n'.format(m_rewards[-1]))\n",
    "            break\n",
    "        print(\"Mean reward (last 100 episodes): {:.3f} \\n\".format(m_rewards[-1]))\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_training = training(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot of the results\n",
    "\n",
    "plt.plot(rewards, label='DQN', linewidth= '1')\n",
    "plt.plot(m_rewards, label='Average reward', color = 'r', linewidth='3')\n",
    "\n",
    "#saving the curves\n",
    "np.savez('rewards_DQN', rewards)\n",
    "np.savez('m_rewards_DQN', m_rewards)\n",
    "\n",
    "plt.title('DQN')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.grid()\n",
    "plt.legend(loc = 4)\n",
    "plt.savefig('DQN_graph.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the trained model\n",
    "agent.model.save('DQN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#at restore, run this method after executing ln[2-3-4-5].\n",
    "agent.model = tf.keras.models.load_model('DQN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "\n",
    "#uncomment 'env.render()' and 'env.display()' to render the episodes\n",
    "\n",
    "state = env.reset()\n",
    "steps = 0\n",
    "eval_time = 60*15\n",
    "starting_time = time.time()\n",
    "stop = lambda : int(time.time() - starting_time) >= eval_time\n",
    "rewards_eval = []\n",
    "ep_reward = 0\n",
    "\n",
    "while True:\n",
    "    state = np.reshape(state, (1, 8))\n",
    "    #returns index of action with maximum value\n",
    "    action = np.argmax(agent.model.predict(state))\n",
    "    #env.render()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    steps += 1\n",
    "    ep_reward += reward\n",
    "    if done:\n",
    "        rewards_eval.append(ep_reward)\n",
    "        print(\"Episode ended in {} time steps\".format(steps))\n",
    "        #env.display()\n",
    "        steps = 0\n",
    "        ep_reward = 0\n",
    "        state = env.reset()\n",
    "        if stop(): \n",
    "            print('Mean reward after the evaluation: {}'.format(np.mean(rewards_eval)))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
