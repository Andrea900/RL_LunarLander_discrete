\begin{algorithm}
	Initialize replay memory $\mathcal{D}$ to capacity $N$\;
	Initialize action-value function $Q$ with random weights\;
	
	\ForEach{episode}{
		Initialize sequence $s_1$ = \{$x_1$\}\\
		\ForEach{step of episode}{
			With probability $\epsilon$ select a random action $a_t$\;
			otherwise select $a_t = \max_a Q(s_t,a;\theta)$\;
			Execute action $a_t$ in emulator and observe reward $r_t$ and next state $x_t_+_1$\;
			Set $s_t_+1 = s_t, a_t, x_t+_1$\;
			Store transition ($s_t, a_t, r_t, s_t_+_1$) in $D$\;
			Sample random minibatch of transitions ($s_j, a_j, r_j, s_j_+_1$)
			from $D$\;
			\uIf {$s_j_+_1$ is terminal state}{
				$y_j = r_j$\;
			}\uElse{
				$y_j = r_j + \gamma\max_aQ(s_t,a;\theta)$
			}
			
			Perform a gradient descent step on $(y_j - Q(s_j, a_j; \theta))^2$
		}
	}
	\caption{Deep Q-Learning with Experience Replay.}
\end{algorithm}
